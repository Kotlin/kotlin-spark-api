package org.jetbrains.kotlinx.spark.examples.streaming

import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.StorageLevels
import org.apache.spark.streaming.Durations
import org.apache.spark.streaming.Time
import org.jetbrains.kotlinx.spark.api.withSparkStreaming
import java.io.Serializable
import java.util.regex.Pattern
import kotlin.system.exitProcess


/**
 * Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
 * network every second.
 *
 * Usage: KotlinSqlNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 * `$ nc -lk 9999`
 * and then run the example
 * `$ bin/run-example org.apache.spark.examples.streaming.KotlinSqlNetworkWordCount localhost 9999`
</port></hostname></port></hostname> */
object KotlinSqlNetworkWordCount {

    private val SPACE = Pattern.compile(" ")

    private const val DEFAULT_IP = "localhost"
    private const val DEFAULT_PORT = "9999"

    @Throws(Exception::class)
    @JvmStatic
    fun main(args: Array<String>) {
        if (args.size < 2 && args.isNotEmpty()) {
            System.err.println("Usage: KotlinNetworkWordCount <hostname> <port>")
            exitProcess(1)
        }

        // Create the context with a 1 second batch size
        withSparkStreaming(
            batchDuration = Durations.seconds(1),
            appName = "KotlinSqlNetworkWordCount",
        ) {


            // Create a KotlinReceiverInputDStream on target ip:port and count the
            // words in input stream of \n delimited text (e.g. generated by 'nc')
            // Note that no duplication in storage level only for running locally.
            // Replication necessary in distributed scenario for fault tolerance.
            val lines = ssc.socketTextStream(
                args.getOrElse(0) { DEFAULT_IP },
                args.getOrElse(1) { DEFAULT_PORT }.toInt(),
                StorageLevels.MEMORY_AND_DISK_SER,
            )
            val words = lines.flatMap { it.split(SPACE).iterator() }

            // Convert RDDs of the words DStream to DataFrame and run SQL query
            words.foreachRDD { rdd: JavaRDD<String>, time: Time ->
                withSpark(rdd) {

                    // Convert JavaRDD<String> to JavaRDD<bean class> to DataFrame (Dataset<Row>)
                    val rowRDD = rdd.map(::KotlinRecord)
                    val wordsDataFrame = rowRDD.toDF()

                    // Creates a temporary view using the DataFrame
                    wordsDataFrame.createOrReplaceTempView("words")

                    // Do word count on table using SQL and print it
                    val wordCountsDataFrame =
                        spark.sql("select word, count(*) as total from words group by word")
                    println("========= $time=========")
                    wordCountsDataFrame.show()
                }
            }
        }
    }
}

data class KotlinRecord(val word: String): Serializable
