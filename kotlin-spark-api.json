{
  "description": "Kotlin for Apache® Spark™",
  "properties": {
    "spark": "3.2",
    "v": "2.0.0"
  },
  "link": "https://github.com/JetBrains/kotlin-spark-api",
  "dependencies": [
    "org.jetbrains.kotlinx.spark:kotlin-spark-api-$spark:$v"
  ],
  "init": [
    "%dumpClassesForSpark",
    "val spark = org.jetbrains.kotlinx.spark.api.SparkSession.builder().master(SparkConf().get(\"spark.master\", \"local[*]\")).appName(\"Jupyter\").getOrCreate()",
    "spark.sparkContext.setLogLevel(org.jetbrains.kotlinx.spark.api.SparkLogLevel.ERROR)",
    "val sc by lazy { org.apache.spark.api.java.JavaSparkContext(spark.sparkContext) }",
    "println(\"Spark session has been started and is running. No `withSpark { }` necessary, you can access `spark` and `sc` directly. To use Spark streaming, use `%use kotlin-spark-api-streaming` instead.\")",

    "inline fun <reified T> List<T>.toDS(): Dataset<T> = toDS(spark)",
    "inline fun <reified T> Array<T>.toDS(): Dataset<T> = spark.dsOf(*this)",
    "inline fun <reified T> dsOf(vararg arg: T): Dataset<T> = spark.dsOf(*arg)",
    "inline fun <reified T> RDD<T>.toDS(): Dataset<T> = toDS(spark)",
    "inline fun <reified T> JavaRDDLike<T, *>.toDS(): Dataset<T> = toDS(spark)",
    "inline fun <reified T> RDD<T>.toDF(): Dataset<Row> = toDF(spark)",
    "inline fun <reified T> JavaRDDLike<T, *>.toDF(): Dataset<Row> = toDF(spark)",
    "val udf: UDFRegistration get() = spark.udf()"
  ],
  "shutdown": [
    "spark.stop()"
  ]
}